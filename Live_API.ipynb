{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Establishing a connection**"
      ],
      "metadata": {
        "id": "JZa7p3J_ZE8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "model = \"gemini-2.5-flash-native-audio-preview-12-2025\"\n",
        "config = {\"response_modalities\": [\"AUDIO\"]}\n",
        "\n",
        "async def main():\n",
        "    async with client.aio.live.connect(model=model, config=config) as session:\n",
        "        print(\"Session started\")\n",
        "        # Send content...\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "9jELzyKhhrJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"Hello, how are you?\"\n",
        "await session.send_client_content(turns=message, turn_complete=True)"
      ],
      "metadata": {
        "id": "XWbqAgQthrGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Incremental content updates**"
      ],
      "metadata": {
        "id": "4elS9jTRnKfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turns = [\n",
        "    {\"role\": \"user\", \"parts\": [{\"text\": \"What is the capital of France?\"}]},\n",
        "    {\"role\": \"model\", \"parts\": [{\"text\": \"Paris\"}]},\n",
        "]\n",
        "\n",
        "await session.send_client_content(turns=turns, turn_complete=False)\n",
        "\n",
        "turns = [{\"role\": \"user\", \"parts\": [{\"text\": \"What is the capital of Germany?\"}]}]\n",
        "\n",
        "await session.send_client_content(turns=turns, turn_complete=True)"
      ],
      "metadata": {
        "id": "ZgEVJw4GZLk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Audio transcriptions**"
      ],
      "metadata": {
        "id": "4o9e-s8Xoj0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "model = \"gemini-2.5-flash-native-audio-preview-12-2025\"\n",
        "\n",
        "config = {\n",
        "    \"response_modalities\": [\"AUDIO\"],\n",
        "    \"output_audio_transcription\": {}\n",
        "}\n",
        "\n",
        "async def main():\n",
        "    async with client.aio.live.connect(model=model, config=config) as session:\n",
        "        message = \"Hello? Gemini are you there?\"\n",
        "\n",
        "        await session.send_client_content(\n",
        "            turns={\"role\": \"user\", \"parts\": [{\"text\": message}]}, turn_complete=True\n",
        "        )\n",
        "\n",
        "        async for response in session.receive():\n",
        "            if response.server_content.model_turn:\n",
        "                print(\"Model turn:\", response.server_content.model_turn)\n",
        "            if response.server_content.output_transcription:\n",
        "                print(\"Transcript:\", response.server_content.output_transcription.text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "qENguANLZLiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To enable transcription of the model's audio input, send input_audio_transcription in setup config.**"
      ],
      "metadata": {
        "id": "hOKhWqW3q23C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from pathlib import Path\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "model = \"gemini-2.5-flash-native-audio-preview-12-2025\"\n",
        "\n",
        "config = {\n",
        "    \"response_modalities\": [\"AUDIO\"],\n",
        "    \"input_audio_transcription\": {},\n",
        "}\n",
        "\n",
        "async def main():\n",
        "    async with client.aio.live.connect(model=model, config=config) as session:\n",
        "        audio_data = Path(\"16000.pcm\").read_bytes()\n",
        "\n",
        "        await session.send_realtime_input(\n",
        "            audio=types.Blob(data=audio_data, mime_type='audio/pcm;rate=16000')\n",
        "        )\n",
        "\n",
        "        async for msg in session.receive():\n",
        "            if msg.server_content.input_transcription:\n",
        "                print('Transcript:', msg.server_content.input_transcription.text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "cc0Fkn3FZLfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change voice and language**\n",
        "\n",
        "To specify a voice, set the voice name within the speechConfig object as part of the session configuration:"
      ],
      "metadata": {
        "id": "ww7CcFECrb12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"response_modalities\": [\"AUDIO\"],\n",
        "    \"speech_config\": {\n",
        "        \"voice_config\": {\"prebuilt_voice_config\": {\"voice_name\": \"Kore\"}}\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "xmpsmsnyZLdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thinking**"
      ],
      "metadata": {
        "id": "NWRYGWHcr87q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gemini-2.5-flash-native-audio-preview-12-2025\"\n",
        "\n",
        "config = types.LiveConnectConfig(\n",
        "    response_modalities=[\"AUDIO\"]\n",
        "    thinking_config=types.ThinkingConfig(\n",
        "        thinking_budget=1024,\n",
        "    )\n",
        ")\n",
        "\n",
        "async with client.aio.live.connect(model=model, config=config) as session:\n",
        "    # Send audio input and receive audio"
      ],
      "metadata": {
        "id": "UV04iw2qZLab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Voice Activity Detection (VAD)**\n",
        "\n",
        "allows the model to recognize when a person is speaking"
      ],
      "metadata": {
        "id": "WuoxTLgmtMcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async for response in session.receive():\n",
        "    if response.server_content.interrupted is True:\n",
        "        # The generation was interrupted\n",
        "\n",
        "        # If realtime playback is implemented in your application,\n",
        "        # you should stop playing audio and clear queued playback here."
      ],
      "metadata": {
        "id": "VFbWA6nWZLXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automatic VAD**\n",
        "\n",
        "By default, the model automatically performs VAD on a continuous audio input stream. VAD can be configured with the realtimeInputConfig.automaticActivityDetection field of the setup configuration."
      ],
      "metadata": {
        "id": "Gu0FcYArwj4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example audio file to try:\n",
        "# URL = \"https://storage.googleapis.com/generativeai-downloads/data/hello_are_you_there.pcm\"\n",
        "# !wget -q $URL -O sample.pcm\n",
        "import asyncio\n",
        "from pathlib import Path\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "model = \"gemini-live-2.5-flash-preview\"\n",
        "\n",
        "config = {\"response_modalities\": [\"TEXT\"]}\n",
        "\n",
        "async def main():\n",
        "    async with client.aio.live.connect(model=model, config=config) as session:\n",
        "        audio_bytes = Path(\"sample.pcm\").read_bytes()\n",
        "\n",
        "        await session.send_realtime_input(\n",
        "            audio=types.Blob(data=audio_bytes, mime_type=\"audio/pcm;rate=16000\")\n",
        "        )\n",
        "\n",
        "        # if stream gets paused, send:\n",
        "        # await session.send_realtime_input(audio_stream_end=True)\n",
        "\n",
        "        async for response in session.receive():\n",
        "            if response.text is not None:\n",
        "                print(response.text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "GOK0-IRXrPcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automatic VAD configuration**"
      ],
      "metadata": {
        "id": "XGUhFlvDundp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "config = {\n",
        "    \"response_modalities\": [\"TEXT\"],\n",
        "    \"realtime_input_config\": {\n",
        "        \"automatic_activity_detection\": {\n",
        "            \"disabled\": False, # default\n",
        "            \"start_of_speech_sensitivity\": types.StartSensitivity.START_SENSITIVITY_LOW,\n",
        "            \"end_of_speech_sensitivity\": types.EndSensitivity.END_SENSITIVITY_LOW,\n",
        "            \"prefix_padding_ms\": 20,\n",
        "            \"silence_duration_ms\": 100,\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "qPpQ5c57rPZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disable automatic VAD**"
      ],
      "metadata": {
        "id": "KFIL-P41uswC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"response_modalities\": [\"TEXT\"],\n",
        "    \"realtime_input_config\": {\"automatic_activity_detection\": {\"disabled\": True}},\n",
        "}\n",
        "\n",
        "async with client.aio.live.connect(model=model, config=config) as session:\n",
        "    # ...\n",
        "    await session.send_realtime_input(activity_start=types.ActivityStart())\n",
        "    await session.send_realtime_input(\n",
        "        audio=types.Blob(data=audio_bytes, mime_type=\"audio/pcm;rate=16000\")\n",
        "    )\n",
        "    await session.send_realtime_input(activity_end=types.ActivityEnd())\n",
        "    # ..."
      ],
      "metadata": {
        "id": "ZNTZvkW3rPW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Token count**\n",
        "\n",
        "You can find the total number of consumed tokens in the usageMetadata field of the returned server message."
      ],
      "metadata": {
        "id": "v4BvkGuJwUYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async for message in session.receive():\n",
        "    # The server will periodically send messages that include UsageMetadata.\n",
        "    if message.usage_metadata:\n",
        "        usage = message.usage_metadata\n",
        "        print(\n",
        "            f\"Used {usage.total_token_count} tokens in total. Response token breakdown:\"\n",
        "        )\n",
        "        for detail in usage.response_tokens_details:\n",
        "            match detail:\n",
        "                case types.ModalityTokenCount(modality=modality, token_count=count):\n",
        "                    print(f\"{modality}: {count}\")"
      ],
      "metadata": {
        "id": "95Hjrcn-rPUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Media resolution**\n",
        "\n",
        "You can specify the media resolution for the input media by setting the mediaResolution field as part of the session configuration:"
      ],
      "metadata": {
        "id": "o1GoJWrawqB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "config = {\n",
        "    \"response_modalities\": [\"AUDIO\"],\n",
        "    \"media_resolution\": types.MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "}"
      ],
      "metadata": {
        "id": "Hl-AfiSZrPR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limitations**\n",
        "\n",
        "Consider the following limitations of the Live API when you plan your project.\n",
        "\n",
        "**Response modalities**\n",
        "\n",
        "\n",
        "You can only set one response modality `(TEXT or AUDIO)` per session in the session configuration. Setting both results in a config error message. This means that you can configure the model to respond with either text or audio, but not both in the same session.\n",
        "\n",
        "**Client authentication**\n",
        "\n",
        "\n",
        "The Live API only provides server-to-server authentication by default. If you're implementing your Live API application using a *client-to-server approach*, you need to use *ephemeral tokens* to mitigate security risks.\n",
        "\n",
        "**Session duration**\n",
        "\n",
        "\n",
        "Audio-only sessions are limited to 15 minutes, and audio plus video sessions are limited to 2 minutes. However, you can configure different session management techniques for unlimited extensions on session duration."
      ],
      "metadata": {
        "id": "oNLavJT7wyw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tool use with Live API**\n",
        "\n",
        "Tool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world and pull in external context while maintaining a real time connection. You can define tools such as Function calling and Google Search with the Live API.\n"
      ],
      "metadata": {
        "id": "9wxYE3QRyY2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function calling**"
      ],
      "metadata": {
        "id": "F39Doua61boh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import wave\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "model = \"gemini-2.5-flash-native-audio-preview-12-2025\"\n",
        "\n",
        "# Simple function definitions\n",
        "turn_on_the_lights = {\"name\": \"turn_on_the_lights\"}\n",
        "turn_off_the_lights = {\"name\": \"turn_off_the_lights\"}\n",
        "\n",
        "tools = [{\"function_declarations\": [turn_on_the_lights, turn_off_the_lights]}]\n",
        "config = {\"response_modalities\": [\"AUDIO\"], \"tools\": tools}\n",
        "\n",
        "async def main():\n",
        "    async with client.aio.live.connect(model=model, config=config) as session:\n",
        "        prompt = \"Turn on the lights please\"\n",
        "        await session.send_client_content(turns={\"parts\": [{\"text\": prompt}]})\n",
        "\n",
        "        wf = wave.open(\"audio.wav\", \"wb\")\n",
        "        wf.setnchannels(1)\n",
        "        wf.setsampwidth(2)\n",
        "        wf.setframerate(24000)  # Output is 24kHz\n",
        "\n",
        "        async for response in session.receive():\n",
        "            if response.data is not None:\n",
        "                wf.writeframes(response.data)\n",
        "            elif response.tool_call:\n",
        "                print(\"The tool was called\")\n",
        "                function_responses = []\n",
        "                for fc in response.tool_call.function_calls:\n",
        "                    function_response = types.FunctionResponse(\n",
        "                        id=fc.id,\n",
        "                        name=fc.name,\n",
        "                        response={ \"result\": \"ok\" } # simple, hard-coded function response\n",
        "                    )\n",
        "                    function_responses.append(function_response)\n",
        "\n",
        "                await session.send_tool_response(function_responses=function_responses)\n",
        "\n",
        "        wf.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "ph9oLGrFrPPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Asynchronous function calling**"
      ],
      "metadata": {
        "id": "EaXsGDuf1g6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-blocking function definitions\n",
        "turn_on_the_lights = {\"name\": \"turn_on_the_lights\", \"behavior\": \"NON_BLOCKING\"} # turn_on_the_lights will run asynchronously\n",
        "turn_off_the_lights = {\"name\": \"turn_off_the_lights\"} # turn_off_the_lights will still pause all interactions with the model"
      ],
      "metadata": {
        "id": "Qj0_sXMErPJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grounding with Google Search**\n",
        "\n",
        "You can enable Grounding with Google Search as part of the session configuration. This increases the Live API's accuracy and prevents hallucinations. See the Grounding tutorial to learn more."
      ],
      "metadata": {
        "id": "ATWbTt2z2Eq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import wave\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "model = \"gemini-2.5-flash-native-audio-preview-12-2025\"\n",
        "\n",
        "tools = [{'google_search': {}}]\n",
        "config = {\"response_modalities\": [\"AUDIO\"], \"tools\": tools}\n",
        "\n",
        "async def main():\n",
        "    async with client.aio.live.connect(model=model, config=config) as session:\n",
        "        prompt = \"When did the last Brazil vs. Argentina soccer match happen?\"\n",
        "        await session.send_client_content(turns={\"parts\": [{\"text\": prompt}]})\n",
        "\n",
        "        wf = wave.open(\"audio.wav\", \"wb\")\n",
        "        wf.setnchannels(1)\n",
        "        wf.setsampwidth(2)\n",
        "        wf.setframerate(24000)  # Output is 24kHz\n",
        "\n",
        "        async for chunk in session.receive():\n",
        "            if chunk.server_content:\n",
        "                if chunk.data is not None:\n",
        "                    wf.writeframes(chunk.data)\n",
        "\n",
        "                # The model might generate and execute Python code to use Search\n",
        "                model_turn = chunk.server_content.model_turn\n",
        "                if model_turn:\n",
        "                    for part in model_turn.parts:\n",
        "                        if part.executable_code is not None:\n",
        "                            print(part.executable_code.code)\n",
        "\n",
        "                        if part.code_execution_result is not None:\n",
        "                            print(part.code_execution_result.output)\n",
        "\n",
        "        wf.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "EKWl6bhNwyDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ephemeral tokens\n",
        "\n",
        "Here's how ephemeral tokens work at a high level:\n",
        "\n",
        "1. Your client (e.g. web app) authenticates with your backend.\n",
        "\n",
        "2. Your backend requests an ephemeral token from Gemini API's provisioning service.\n",
        "\n",
        "3. Gemini API issues a short-lived token.\n",
        "\n",
        "4. Your backend sends the token to the client for WebSocket connections to Live API. You can do this by swapping your API key with an ephemeral token.\n",
        "\n",
        "5. The client then uses the token as if it were an API key.\n"
      ],
      "metadata": {
        "id": "Bf-AYZKXHu7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "now = datetime.datetime.now(tz=datetime.timezone.utc)\n",
        "\n",
        "client = genai.Client(\n",
        "    http_options={'api_version': 'v1alpha',}\n",
        ")\n",
        "\n",
        "token = client.auth_tokens.create(\n",
        "    config = {\n",
        "    'uses': 1, # The ephemeral token can only be used to start a single session\n",
        "    'expire_time': now + datetime.timedelta(minutes=30), # Default is 30 minutes in the future\n",
        "    # 'expire_time': '2025-05-17T00:00:00Z',   # Accepts isoformat.\n",
        "    'new_session_expire_time': now + datetime.timedelta(minutes=1), # Default 1 minute in the future\n",
        "    'http_options': {'api_version': 'v1alpha'},\n",
        "  }\n",
        ")\n",
        "\n",
        "# You'll need to pass the value under token.name back to your client to use it"
      ],
      "metadata": {
        "id": "LL1wX4GQwx-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect to Live API with an ephemeral token**"
      ],
      "metadata": {
        "id": "QkWhz4l6IKiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import { GoogleGenAI, Modality } from '@google/genai';\n",
        "\n",
        "// Use the token generated in the \"Create an ephemeral token\" section here\n",
        "const ai = new GoogleGenAI({\n",
        "  apiKey: token.name\n",
        "});\n",
        "const model = 'gemini-2.5-flash-native-audio-preview-12-2025';\n",
        "const config = { responseModalities: [Modality.AUDIO] };\n",
        "\n",
        "async function main() {\n",
        "\n",
        "  const session = await ai.live.connect({\n",
        "    model: model,\n",
        "    config: config,\n",
        "    callbacks: { ... },\n",
        "  });\n",
        "\n",
        "  // Send content...\n",
        "\n",
        "  session.close();\n",
        "}\n",
        "\n",
        "main();"
      ],
      "metadata": {
        "id": "2qUkisl8wx7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u0WhSLJrwx4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-7AEnTAGwx1Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}